{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red neuronal que armamos no cuenta con capas intermedias/ocultas. Esta la de entrada, que es un dataset descargado de Kaggle, y la salida, que intenta predecir el tipo de conductor(por ejemplo, casual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la funcion de activación sigmoide, ideal para clasificación binaria ya que devuelve valores entre 1 y 0, que se puede interpretar como que tan probable es que un conjunto de muetra pertenezca a una clase.\n",
    "# Esta funcion de activación es la que se encontrará en la neurona de salida\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaramos dos tipos de funciones diferentes para calcular el error. La linear, que resta las predicciones a los valores reales, y la mean absolute, que calcula el error e ignora en que dirección\n",
    "# fue este error(si es negativo o positivo)\n",
    "def linear_error(y_true, y_pred):\n",
    "    return np.mean(y_true - y_pred)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que randomiza los weights y los bias iniciales\n",
    "def initialize_weights(input_size, output_size):\n",
    "    weights = np.random.randn(input_size, output_size) * 0.01\n",
    "    bias = np.zeros((1, output_size))\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion que hace la propagacion hacia adelante, myultiplicando las X por sus weights y luego sumando el bias. Y al final, pasando el resultado por la funcion de activacion sigmoide\n",
    "def forward_pass(X, weights, bias):\n",
    "    z = np.dot(X, weights) + bias\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la funcion de propagacion hacia atras para bque pueda ajustar los weights y bias aprendiendo de su error\n",
    "def backpropagation(X, y, output, weights, bias, learning_rate):\n",
    "    # Calculamos el error entre la salida esperada y la salida real (output)\n",
    "    # `error` representa cuán lejos está la predicción de la verdad\n",
    "    error = output - y\n",
    "    \n",
    "    # Calculamos el gradiente de los weights\n",
    "    # `sigmoid_derivative(output)` calcula la derivada de la función de activación \n",
    "    # `X.T` es la transposición de la matriz de entrada, lo que permite realizar el producto punto correctamente\n",
    "    d_weights = np.dot(X.T, error * sigmoid_derivative(output))\n",
    "\n",
    "    # Calculamos el gradiente del bias\n",
    "    # Sumamos el error ajustado por la derivada de la función de activación a lo largo de las filas\n",
    "    d_bias = np.sum(error * sigmoid_derivative(output), axis=0, keepdims=True)\n",
    "    \n",
    "    # Ajustamos los weights y bias usando la tasa de aprendizaje\n",
    "    # `learning_rate` controla cuánto se ajustan los pesos en cada iteración\n",
    "    weights -= learning_rate * d_weights\n",
    "    bias -= learning_rate * d_bias\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la funcion que entrenara a la NN\n",
    "def train(X, y, input_size, output_size, epochs, learning_rate):\n",
    "    # Usamos la funcion previamente crada oara crear pesos y bias\n",
    "    weights, bias = initialize_weights(input_size, output_size)\n",
    "    \n",
    "    # Entrenamos por varias epochs (veces que se hará todo el proceso de ir para delante, para atras y actualizar weights y bias)\n",
    "    for epoch in range(epochs):\n",
    "        # Propagación hacia adelante\n",
    "        output = forward_pass(X, weights, bias)\n",
    "        \n",
    "        # Calculo de pérdida\n",
    "        loss = linear_error(y, output)\n",
    "        \n",
    "        # Retropropagación\n",
    "        weights, bias = backpropagation(X, y, output, weights, bias, learning_rate)\n",
    "        \n",
    "        # Imprime el progreso cada 100 épocas para chequear que tan bien o mal va el aprendizaje\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cra la funcion con la que se va a predecir \n",
    "def predict(X, weights, bias):\n",
    "    output = forward_pass(X, weights, bias)\n",
    "    # return (output > 0.3).astype(int)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #x = ([precio,metroscuadrados,pisomarmol,pisomadera,hornoagas,hornoelectrico])\n",
    "# #y = ([moderna,rustica,barata])\n",
    "\n",
    "# X = np.array([\n",
    "#     [150000, 120, 1, 0, 1, 0],\n",
    "#     [80000, 80, 0, 1, 0, 1],\n",
    "#     [200000, 150, 1, 1, 1, 1],\n",
    "#     [50000, 60, 0, 0, 0, 1],\n",
    "#     [120000, 100, 1, 0, 1, 0]\n",
    "# ])\n",
    "\n",
    "\n",
    "# y = np.array([\n",
    "#     [1, 0, 0],\n",
    "#     [0, 1, 1],\n",
    "#     [1, 0, 0],\n",
    "#     [0, 1, 1],\n",
    "#     [1, 0, 0] \n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red tiene X cantidad de entradas (desconzco la cantidad de filas). Cada entrada(fila en el caso del primer modelo) va a tener su respectivo weight y va a haber un bias para la capa.\n",
    "En el caso del modelo con capas ocultas, cada entrada (tanto las del dataset como las creadas por el modelo) va a tener su respectivo weight y va a haber un bias por cada neurona de salida de la capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicle Model</th>\n",
       "      <th>Battery Capacity (kWh)</th>\n",
       "      <th>Energy Consumed (kWh)</th>\n",
       "      <th>Charging Duration (hours)</th>\n",
       "      <th>Charging Rate (kW)</th>\n",
       "      <th>Charging Cost (USD)</th>\n",
       "      <th>State of Charge (Start %)</th>\n",
       "      <th>State of Charge (End %)</th>\n",
       "      <th>Distance Driven (since last charge) (km)</th>\n",
       "      <th>Temperature (°C)</th>\n",
       "      <th>Vehicle Age (years)</th>\n",
       "      <th>User Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW i3</td>\n",
       "      <td>108.463007</td>\n",
       "      <td>60.712346</td>\n",
       "      <td>0.591363</td>\n",
       "      <td>36.389181</td>\n",
       "      <td>13.087717</td>\n",
       "      <td>29.371576</td>\n",
       "      <td>86.119962</td>\n",
       "      <td>293.602111</td>\n",
       "      <td>27.947953</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Commuter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vehicle Model  Battery Capacity (kWh)  Energy Consumed (kWh)  \\\n",
       "0        BMW i3              108.463007              60.712346   \n",
       "\n",
       "   Charging Duration (hours)  Charging Rate (kW)  Charging Cost (USD)  \\\n",
       "0                   0.591363           36.389181            13.087717   \n",
       "\n",
       "   State of Charge (Start %)  State of Charge (End %)  \\\n",
       "0                  29.371576                86.119962   \n",
       "\n",
       "   Distance Driven (since last charge) (km)  Temperature (°C)  \\\n",
       "0                                293.602111         27.947953   \n",
       "\n",
       "   Vehicle Age (years) User Type  \n",
       "0                  2.0  Commuter  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##https://www.kaggle.com/datasets/valakhorasani/electric-vehicle-charging-patterns\n",
    "\n",
    "#Importamos un dataset y sacamos filas con valores null y ciertas columnas que consideramos innecearias\n",
    "dataset = pd.read_csv(\"ev_charging_patterns.csv\")\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.drop(columns=['User ID', 'Charging Station ID', 'Charging Station Location','Charging Start Time','Charging End Time','Time of Day','Day of Week','Charger Type'])\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nos guardamos todas las columnas excepto la que intentamos predecir en una variable X\n",
    "X = dataset.drop('User Type', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=False).astype(int).to_numpy()\n",
    "\n",
    "#Guardamos la variable a predecir en una llamada Y\n",
    "y = dataset['User Type']\n",
    "y= pd.get_dummies(y, drop_first=False).astype(int).to_numpy()\n",
    "y\n",
    "\n",
    "#Ambas las tranformamos a matrices de numpy para poder manipularlas con las funciones de arriba y las que siguen abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fiteamos X e Y para minimizar el error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponemos los hiperparametros\n",
    "input_size = X.shape[1]\n",
    "output_size = y.shape[1]\n",
    "epochs = 3000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -0.5000\n",
      "Epoch 100, Loss: -0.0599\n",
      "Epoch 200, Loss: -0.0539\n",
      "Epoch 300, Loss: -0.0514\n",
      "Epoch 400, Loss: -0.0499\n",
      "Epoch 500, Loss: -0.0492\n",
      "Epoch 600, Loss: -0.0488\n",
      "Epoch 700, Loss: -0.0484\n",
      "Epoch 800, Loss: -0.0481\n",
      "Epoch 900, Loss: -0.0480\n",
      "Epoch 1000, Loss: -0.0484\n",
      "Epoch 1100, Loss: -0.0490\n",
      "Epoch 1200, Loss: -0.0495\n",
      "Epoch 1300, Loss: -0.0496\n",
      "Epoch 1400, Loss: -0.0497\n",
      "Epoch 1500, Loss: -0.0496\n",
      "Epoch 1600, Loss: -0.0496\n",
      "Epoch 1700, Loss: -0.0496\n",
      "Epoch 1800, Loss: -0.0495\n",
      "Epoch 1900, Loss: -0.0495\n",
      "Epoch 2000, Loss: -0.0495\n",
      "Epoch 2100, Loss: -0.0494\n",
      "Epoch 2200, Loss: -0.0494\n",
      "Epoch 2300, Loss: -0.0494\n",
      "Epoch 2400, Loss: -0.0494\n",
      "Epoch 2500, Loss: -0.0494\n",
      "Epoch 2600, Loss: -0.0493\n",
      "Epoch 2700, Loss: -0.0493\n",
      "Epoch 2800, Loss: -0.0493\n",
      "Epoch 2900, Loss: -0.0493\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos al modelo\n",
    "\n",
    "weights, bias = train(X, y, input_size, output_size, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones:\n",
      "[[3.01279126e-05 1.31053130e-05 8.02940916e-09]\n",
      " [1.01779578e-05 1.62507603e-10 1.07975407e-08]\n",
      " [4.07795799e-08 5.70703846e-07 5.06889133e-04]\n",
      " ...\n",
      " [8.90666795e-12 4.36076440e-08 5.67055932e-10]\n",
      " [4.79712893e-12 9.86434330e-01 1.99329277e-04]\n",
      " [2.23098467e-13 5.07623344e-19 5.92758148e-09]]\n",
      "Salidas reales:\n",
      "[[-0.66390084  1.34145628 -0.71274119]\n",
      " [ 1.50624903 -0.74545851 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " ...\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X, weights, bias)\n",
    "print(\"Predicciones:\")\n",
    "print(predictions)\n",
    "\n",
    "# Salidas reales para comparación\n",
    "print(\"Salidas reales:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9684770401103653"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = np.mean((y - predictions) ** 2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para agregar  k capas ocultas con n neuronas cada una hay que hacer modificaciones en la funcion en la que se crean los weights y los bias, al igual que en la de forward propagation. Además, yo voy a usar otra funcion de activacion(ReLu) para las capas intermedias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos otra funcion de activacion para las capas ocultas que pide el bonus \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_initialize_weights(layer_sizes):\n",
    "    # Creamos arrays vacios para almacenar los weights y bias\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    # Crea valores random para todas las capas\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        \n",
    "        # Agrega los pesos y sesgos a sus respectivas listas\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "    \n",
    "    # Devuelve los arrays llenos\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_forward_pass(X, weights, biases, layer_sizes):\n",
    "    activations = [X]  # Valores que se utilizan como entrada para la siguiente capa\n",
    "\n",
    "    # Bucle para recorrer las capas ocultas (excluye la última capa para oder utilizar otra funcion de activacion)\n",
    "    for i in range(len(layer_sizes) - 2):  # -2 porque la última capa se trata por separado\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        a = relu(z)  # Aplicar función de activación ReLU a la salida de la capa actual\n",
    "        activations.append(a)  # Añadir el valor calculado al array para que lo utilice la siguiente capa\n",
    "\n",
    "    # Cálculo para la capa de salida\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    output = sigmoid(z)\n",
    "    activations.append(output)\n",
    "\n",
    "    # Devolver todos los valores calculados\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_backpropagation(X, y, activations, weights, biases, learning_rate):\n",
    "    m = y.shape[0]  # Número de ejemplos\n",
    "    L = len(weights)  # Número de capas\n",
    "    \n",
    "    # Se calcula el error y cuanto debe ajustarse de la capa de salida\n",
    "    delta_z = activations[-1] - y \n",
    "    delta_weights = np.dot(activations[-2].T, delta_z) / m \n",
    "    delta_bias = np.sum(delta_z, axis=0, keepdims=True) / m \n",
    "\n",
    "    # Actualizar weights y bias de la capa de salida\n",
    "    weights[-1] -= learning_rate * delta_weights\n",
    "    biases[-1] -= learning_rate * delta_bias\n",
    "\n",
    "    for l in range(L - 2, -1, -1):  # Desde la última capa oculta (L-2) hasta la primera capa (0)\n",
    "    \n",
    "        # Multiplicamos el gradiente de la capa siguiente (delta_z) por la transpuesta de los pesos de la capa actual (l+1).\n",
    "        # Luego, multiplicamos por la derivada de la activación ReLU para ajustar el gradiente en la capa actual.\n",
    "        delta_z = np.dot(delta_z, weights[l + 1].T) * relu_derivative(activations[l + 1])\n",
    "        \n",
    "        # Se hace el producto punto entre las activaciones transpuestas de la capa actual y delta_z.\n",
    "        # El resultado se divide por el número de ejemplos en el lote (m) para obtener el promedio del gradiente.\n",
    "        delta_weights = np.dot(activations[l].T, delta_z) / m\n",
    "        \n",
    "        # Sumamos todos los gradientes delta_z de la capa actual y promediamos dividiendo por el número de ejemplos (m).\n",
    "        delta_bias = np.sum(delta_z, axis=0, keepdims=True) / m\n",
    "        \n",
    "        #Se actualizan los weights y bias de cada capa\n",
    "        weights[l] -= learning_rate * delta_weights\n",
    "        biases[l] -= learning_rate * delta_bias\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_train(X, y, layer_sizes, epochs=1000, learning_rate=0.01):\n",
    "\n",
    "    weights, biases = bonus_initialize_weights(layer_sizes)\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        activations = bonus_forward_pass(X, weights, biases, layer_sizes)\n",
    "        \n",
    "        loss = linear_error(y, activations[-1])\n",
    "\n",
    "        weights, biases = bonus_backpropagation(X, y, activations, weights, biases, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_predict(X, weights, biases, layer_sizes):\n",
    "    activations = bonus_forward_pass(X, weights, biases, layer_sizes)\n",
    "    return activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -0.5000\n",
      "Epoch 100, Loss: -0.3909\n",
      "Epoch 200, Loss: -0.3116\n",
      "Epoch 300, Loss: -0.2545\n",
      "Epoch 400, Loss: -0.2129\n",
      "Epoch 500, Loss: -0.1818\n",
      "Epoch 600, Loss: -0.1579\n",
      "Epoch 700, Loss: -0.1392\n",
      "Epoch 800, Loss: -0.1242\n",
      "Epoch 900, Loss: -0.1119\n",
      "Epoch 1000, Loss: -0.1017\n",
      "Epoch 1100, Loss: -0.0932\n",
      "Epoch 1200, Loss: -0.0859\n",
      "Epoch 1300, Loss: -0.0796\n",
      "Epoch 1400, Loss: -0.0742\n",
      "Epoch 1500, Loss: -0.0694\n",
      "Epoch 1600, Loss: -0.0652\n",
      "Epoch 1700, Loss: -0.0614\n",
      "Epoch 1800, Loss: -0.0581\n",
      "Epoch 1900, Loss: -0.0550\n",
      "Epoch 2000, Loss: -0.0523\n",
      "Epoch 2100, Loss: -0.0498\n",
      "Epoch 2200, Loss: -0.0476\n",
      "Epoch 2300, Loss: -0.0455\n",
      "Epoch 2400, Loss: -0.0436\n",
      "Epoch 2500, Loss: -0.0419\n",
      "Epoch 2600, Loss: -0.0403\n",
      "Epoch 2700, Loss: -0.0388\n",
      "Epoch 2800, Loss: -0.0374\n",
      "Epoch 2900, Loss: -0.0361\n",
      "Epoch 3000, Loss: -0.0349\n",
      "Epoch 3100, Loss: -0.0337\n",
      "Epoch 3200, Loss: -0.0327\n",
      "Epoch 3300, Loss: -0.0317\n",
      "Epoch 3400, Loss: -0.0307\n",
      "Epoch 3500, Loss: -0.0298\n",
      "Epoch 3600, Loss: -0.0290\n",
      "Epoch 3700, Loss: -0.0282\n",
      "Epoch 3800, Loss: -0.0274\n",
      "Epoch 3900, Loss: -0.0267\n",
      "Epoch 4000, Loss: -0.0260\n",
      "Epoch 4100, Loss: -0.0254\n",
      "Epoch 4200, Loss: -0.0248\n",
      "Epoch 4300, Loss: -0.0242\n",
      "Epoch 4400, Loss: -0.0236\n",
      "Epoch 4500, Loss: -0.0231\n",
      "Epoch 4600, Loss: -0.0226\n",
      "Epoch 4700, Loss: -0.0221\n",
      "Epoch 4800, Loss: -0.0216\n",
      "Epoch 4900, Loss: -0.0212\n",
      "Epoch 5000, Loss: -0.0208\n",
      "Epoch 5100, Loss: -0.0203\n",
      "Epoch 5200, Loss: -0.0200\n",
      "Epoch 5300, Loss: -0.0196\n",
      "Epoch 5400, Loss: -0.0192\n",
      "Epoch 5500, Loss: -0.0188\n",
      "Epoch 5600, Loss: -0.0185\n",
      "Epoch 5700, Loss: -0.0182\n",
      "Epoch 5800, Loss: -0.0179\n",
      "Epoch 5900, Loss: -0.0175\n",
      "Epoch 6000, Loss: -0.0172\n",
      "Epoch 6100, Loss: -0.0170\n",
      "Epoch 6200, Loss: -0.0167\n",
      "Epoch 6300, Loss: -0.0164\n",
      "Epoch 6400, Loss: -0.0162\n",
      "Epoch 6500, Loss: -0.0159\n",
      "Epoch 6600, Loss: -0.0157\n",
      "Epoch 6700, Loss: -0.0154\n",
      "Epoch 6800, Loss: -0.0152\n",
      "Epoch 6900, Loss: -0.0150\n",
      "Epoch 7000, Loss: -0.0147\n",
      "Epoch 7100, Loss: -0.0145\n",
      "Epoch 7200, Loss: -0.0143\n",
      "Epoch 7300, Loss: -0.0141\n",
      "Epoch 7400, Loss: -0.0139\n",
      "Epoch 7500, Loss: -0.0137\n",
      "Epoch 7600, Loss: -0.0136\n",
      "Epoch 7700, Loss: -0.0134\n",
      "Epoch 7800, Loss: -0.0132\n",
      "Epoch 7900, Loss: -0.0130\n",
      "Epoch 8000, Loss: -0.0129\n",
      "Epoch 8100, Loss: -0.0127\n",
      "Epoch 8200, Loss: -0.0126\n",
      "Epoch 8300, Loss: -0.0124\n",
      "Epoch 8400, Loss: -0.0123\n",
      "Epoch 8500, Loss: -0.0121\n",
      "Epoch 8600, Loss: -0.0120\n",
      "Epoch 8700, Loss: -0.0118\n",
      "Epoch 8800, Loss: -0.0117\n",
      "Epoch 8900, Loss: -0.0116\n",
      "Epoch 9000, Loss: -0.0114\n",
      "Epoch 9100, Loss: -0.0113\n",
      "Epoch 9200, Loss: -0.0112\n",
      "Epoch 9300, Loss: -0.0110\n",
      "Epoch 9400, Loss: -0.0109\n",
      "Epoch 9500, Loss: -0.0108\n",
      "Epoch 9600, Loss: -0.0107\n",
      "Epoch 9700, Loss: -0.0106\n",
      "Epoch 9800, Loss: -0.0105\n",
      "Epoch 9900, Loss: -0.0104\n"
     ]
    }
   ],
   "source": [
    "input_size = X.shape[1]\n",
    "output_size = y.shape[1]\n",
    "\n",
    "#Cantidad de neuronas por capa. Cada vez que se agrega un numero entre input y output size es una nueva capa, y el numero agregado representa la cantidad de neuronas que tendrá\n",
    "layer_sizes = [input_size, 6,3,2, output_size]\n",
    "\n",
    "epochs = 10000\n",
    "learning_rate = 0.01\n",
    "weights, biases = bonus_train(X, y, layer_sizes, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones:\n",
      "[[0.01026163 0.01026163 0.01026163]\n",
      " [0.01026163 0.01026163 0.01026163]\n",
      " [0.01026163 0.01026163 0.01026163]\n",
      " ...\n",
      " [0.01026163 0.01026163 0.01026163]\n",
      " [0.01026163 0.01026163 0.01026163]\n",
      " [0.01026163 0.01026163 0.01026163]]\n",
      "Salidas reales:\n",
      "[[-0.66390084  1.34145628 -0.71274119]\n",
      " [ 1.50624903 -0.74545851 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " ...\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]\n",
      " [-0.66390084  1.34145628 -0.71274119]]\n"
     ]
    }
   ],
   "source": [
    "bonus_predictions = bonus_predict(X, weights, biases, layer_sizes)\n",
    "print(\"Predicciones:\")\n",
    "print(bonus_predictions)\n",
    "\n",
    "print(\"Salidas reales:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que yo haría para que la salida sea mutualmente excluyente es cambiar la funcion de activación final. En vez de usar sigmoide, yo utilizaría SoftMax. Lo que esta funcion de activacion hace es que la suma de todas las columnas de la salida den 1. Al hacer esto, se puede interpretar como la probabilidad de que pertenezca a cada clase.\n",
    "\n",
    " En el dataset de tipos de conductor por ejemplo, haria que en vez de devolverme un rango entre 0 y 1 para cada posiblidad (Commuter, Long-Distance Traveler) me devuelva numeros que sumando todas las columnas den 1, haciendo que cada uno represente la posibilidad de que el conjunto original (X) perteneza a cada columna. Una vez tenga las \"probabilidades\", la que mayor chance tenga será a la que pertenece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
